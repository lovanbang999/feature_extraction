"""
WEB APP FEATURE EXTRACTION DEMO
Streamlit application t√≠ch h·ª£p 4 ph∆∞∆°ng ph√°p feature extraction
"""

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import sys
import os

# Custom CSS
st.markdown("""
<style>
    .main > div {
        padding-top: 2rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #FF4B4B;
        color: white;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #FF6B6B;
    }
    h1 {
        color: #FF4B4B;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #FF4B4B;
    }
</style>
""", unsafe_allow_html=True)

# Add utils to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from dict_features import DictFeatureExtractor
from hash_features import HashFeatureExtractor
from text_features import TextFeatureExtractor
from image_features import ImageFeatureExtractor

# Page config
st.set_page_config(
    page_title="Feature Extraction Demo",
    page_icon="üöÄ",
    layout="wide"
)

with st.expander("üë• Th√¥ng tin nh√≥m", expanded=False):
    st.write("**Nh√≥m 8 - M√¥n Khai Ph√° D·ªØ Li·ªáu**")
    
    team_members = [
        {'STT': 1, 'H·ªç t√™n': 'L√≤ VƒÉn B·∫±ng', 'MSSV': '2251061721', 'Ph·∫ßn ƒë·∫£m nh·∫≠n': 'Image Features'},
        {'STT': 2, 'H·ªç t√™n': 'Nguy·ªÖn Trung Ki√™n', 'MSSV': '2251061811', 'Ph·∫ßn ƒë·∫£m nh·∫≠n': 'Dict Features'},
        {'STT': 3, 'H·ªç t√™n': 'Thi·ªÅu B√° Vi·ªát', 'MSSV': '2251061924', 'Ph·∫ßn ƒë·∫£m nh·∫≠n': 'Text Features'},
        {'STT': 4, 'H·ªç t√™n': 'L∆∞·ªùng VƒÉn C∆∞∆°ng', 'MSSV': '20210004', 'Ph·∫ßn ƒë·∫£m nh·∫≠n': 'Feature Hashing'}
    ]
    
    df_team = pd.DataFrame(team_members)
    st.dataframe(df_team, hide_index=True, use_container_width=True)

# Title
st.title("üöÄ Feature Extraction Demo")
st.markdown("**Nh√≥m 8 - M√¥n Khai Ph√° D·ªØ Li·ªáu**")
st.markdown("---")

# Sidebar
st.sidebar.title("üìã Navigation")
page = st.sidebar.radio(
    "Ch·ªçn ph∆∞∆°ng ph√°p:",
    ["üè† T·ªïng quan", "üìä Dict Features", "# Feature Hashing", 
     "üìù Text Features", "üñºÔ∏è Image Features", "üî¨ So s√°nh", "üéØ ML Performance"]
)

# ==================== TRANG T·ªîNG QUAN ====================
if page == "üè† T·ªïng quan":
    st.header("Gi·ªõi thi·ªáu Feature Extraction")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìñ Feature Extraction l√† g√¨?")
        st.write("""
        Feature Extraction (Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng) l√† qu√° tr√¨nh chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ 
        (raw data) th√†nh d·∫°ng s·ªë (numerical) m√† c√°c thu·∫≠t to√°n machine learning c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.
        
        **T·∫°i sao c·∫ßn Feature Extraction?**
        - Machine learning ch·ªâ l√†m vi·ªác v·ªõi s·ªë
        - Gi·∫£m chi·ªÅu d·ªØ li·ªáu (dimensionality reduction)
        - Gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng
        - C·∫£i thi·ªán hi·ªáu su·∫•t model
        """)
    
    with col2:
        st.subheader("üéØ 4 Ph∆∞∆°ng ph√°p trong Demo")
        st.write("""
        1. **Dict Features**: Chuy·ªÉn dictionary ‚Üí vector
        2. **Feature Hashing**: Hash trick cho high-dimensional data
        3. **Text Features**: Chuy·ªÉn text ‚Üí vector (TF-IDF, Count)
        4. **Image Features**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh
        """)
    
    st.markdown("---")
    st.subheader("üìä Quy tr√¨nh chung")
    st.image("https://miro.medium.com/max/1400/1*VQvV5kVXZvHlmBDxWcZvNg.png", 
             caption="Feature Extraction Pipeline", use_column_width=True)
    
    st.info("üëà Ch·ªçn ph∆∞∆°ng ph√°p ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu demo!")

# ==================== DICT FEATURES ====================
elif page == "üìä Dict Features":
    st.header("7.2.1. Loading Features from Dicts")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    st.markdown("""
    **DictVectorizer** chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu d·∫°ng dictionary th√†nh feature vectors.
    
    **V√≠ d·ª•:** """)
    st.code("""
    Input:  [{'age': '25', 'city': 'Hanoi'}, {'age': '30', 'city': 'Danang'}]
    Output: [[25, 1, 0], [30, 0, 1]]  # [age, city=Hanoi, city=Danang]""",language='python')

    st.markdown("""
                **·ª®ng d·ª•ng:** D·ªØ li·ªáu categorical nh∆∞ th√¥ng tin kh√°ch h√†ng, s·∫£n ph·∫©m, v.v. 
                """)
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Ch·ªçn mode
    demo_mode = st.radio("Ch·ªçn mode:", ["V√≠ d·ª• ƒë∆°n gi·∫£n", "Titanic Dataset"])
    
    if demo_mode == "V√≠ d·ª• ƒë∆°n gi·∫£n":
        st.write("**Nh·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n:**")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            name1 = st.text_input("T√™n 1:", "An")
            city1 = st.selectbox("Th√†nh ph·ªë 1:", ["Hanoi", "HCM", "Danang"], key="city1")
            age1 = st.number_input("Tu·ªïi 1:", 18, 100, 25)
        
        with col2:
            name2 = st.text_input("T√™n 2:", "Binh")
            city2 = st.selectbox("Th√†nh ph·ªë 2:", ["Hanoi", "HCM", "Danang"], key="city2")
            age2 = st.number_input("Tu·ªïi 2:", 18, 100, 30)
        
        with col3:
            name3 = st.text_input("T√™n 3:", "Chi")
            city3 = st.selectbox("Th√†nh ph·ªë 3:", ["Hanoi", "HCM", "Danang"], key="city3")
            age3 = st.number_input("Tu·ªïi 3:", 18, 100, 22)
        
        if st.button("üöÄ Extract Features"):
            data = [
                {'name': name1, 'city': city1, 'age': age1},
                {'name': name2, 'city': city2, 'age': age2},
                {'name': name3, 'city': city3, 'age': age3}
            ]
            
            extractor = DictFeatureExtractor()
            features, feature_names = extractor.extract_features(data)
            
            st.success("‚úÖ Extraction ho√†n t·∫•t!")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.write("**Input Data:**")
                st.json(data)
            
            with col_b:
                st.write("**Feature Names:**")
                st.write(list(feature_names))
            
            st.write("**Feature Matrix:**")
            df_features = pd.DataFrame(features, columns=feature_names)
            st.dataframe(df_features, use_container_width=True)
            
            st.write(f"üìè **Shape:** {features.shape} (3 samples √ó {features.shape[1]} features)")
    
    else:  # Titanic Dataset
        st.write("**Demo v·ªõi Titanic Dataset:**")
        
        try:
            df = pd.read_csv('datasets/titanic.csv')
            st.write("Dataset preview:")
            st.dataframe(df.head(), use_container_width=True)
            
            n_samples = st.slider("S·ªë l∆∞·ª£ng samples:", 5, 50, 10)
            
            if st.button("üöÄ Extract Features from Titanic"):
                extractor = DictFeatureExtractor()
                dict_data, features, feature_names = extractor.demo_with_titanic(df, n_samples)
                
                st.success(f"‚úÖ ƒê√£ extract {n_samples} samples!")
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.write("**Original Data (dict format):**")
                    st.json(dict_data[:3])  # Show first 3
                
                with col_b:
                    st.write("**Feature Names:**")
                    st.write(list(feature_names))
                
                st.write("**Feature Matrix:**")
                df_features = pd.DataFrame(features, columns=feature_names)
                st.dataframe(df_features, use_container_width=True)
                
                st.write(f"üìè **Shape:** {features.shape}")
        
        except FileNotFoundError:
            st.error("‚ùå Kh√¥ng t√¨m th·∫•y datasets/titanic.csv. Vui l√≤ng ch·∫°y download_data.py tr∆∞·ªõc!")

# ==================== FEATURE HASHING ====================
elif page == "# Feature Hashing":
    st.header("7.2.2. Feature Hashing")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    st.write("""
    **Feature Hashing** (Hashing Trick) s·ª≠ d·ª•ng hash function ƒë·ªÉ chuy·ªÉn features th√†nh vector 
    c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh.
    
    **∆Øu ƒëi·ªÉm:**
    - ‚ö° R·∫•t nhanh (kh√¥ng c·∫ßn l∆∞u vocabulary)
    - üíæ Ti·∫øt ki·ªám b·ªô nh·ªõ
    - üîÑ X·ª≠ l√Ω ƒë∆∞·ª£c unseen features
    
    **Nh∆∞·ª£c ƒëi·ªÉm:**
    - ‚ö†Ô∏è Hash collision (nhi·ªÅu features ‚Üí c√πng 1 hash value)
    - ‚ùì Kh√¥ng bi·∫øt feature g·ªëc l√† g√¨ (one-way)
    """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        country1 = st.text_input("Country 1:", "Vietnam")
        city1 = st.text_input("City 1:", "Hanoi")
        age1 = st.number_input("Age 1:", 18, 100, 25, key="hash_age1")
    
    with col2:
        country2 = st.text_input("Country 2:", "Thailand")
        city2 = st.text_input("City 2:", "Bangkok")
        age2 = st.number_input("Age 2:", 18, 100, 28, key="hash_age2")
    
    with col3:
        country3 = st.text_input("Country 3:", "Vietnam")
        city3 = st.text_input("City 3:", "HCM")
        age3 = st.number_input("Age 3:", 18, 100, 30, key="hash_age3")
    
    n_features = st.slider("S·ªë features sau khi hash:", 5, 20, 10)
    
    if st.button("üöÄ Extract & Compare"):
        data = [
            {'country': country1, 'city': city1, 'age': age1},
            {'country': country2, 'city': city2, 'age': age2},
            {'country': country3, 'city': city3, 'age': age3}
        ]
        
        extractor = HashFeatureExtractor(n_features=n_features)
        comparison, dict_features, hash_features = extractor.compare_with_dict_vectorizer(data)
        
        st.success("‚úÖ Extraction ho√†n t·∫•t!")
        
        # Show original data
        st.write("**Input Data:**")
        st.json(data)
        
        # Comparison table
        st.subheader("üìä So s√°nh DictVectorizer vs FeatureHasher")
        
        comp_df = pd.DataFrame({
            'Method': ['DictVectorizer', 'FeatureHasher'],
            'Number of Features': [comparison['DictVectorizer']['n_features'], 
                                   comparison['FeatureHasher']['n_features']],
            'Memory (bytes)': [comparison['DictVectorizer']['memory_size'],
                              comparison['FeatureHasher']['memory_size']]
        })
        st.dataframe(comp_df, use_container_width=True)
        
        # Show features
        col_a, col_b = st.columns(2)
        
        with col_a:
            st.write("**DictVectorizer Output:**")
            st.write(dict_features)
            st.caption(f"Shape: {dict_features.shape}")
        
        with col_b:
            st.write("**FeatureHasher Output:**")
            st.write(hash_features)
            st.caption(f"Shape: {hash_features.shape}")
        
        st.info("""
        **üí° Nh·∫≠n x√©t:**
        - FeatureHasher c√≥ s·ªë features c·ªë ƒë·ªãnh (b·∫°n ch·ªçn)
        - DictVectorizer c√≥ s·ªë features = s·ªë unique values
        - FeatureHasher ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n khi c√≥ nhi·ªÅu categorical values
        """)

# ==================== TEXT FEATURES ====================
elif page == "üìù Text Features":
    st.header("7.2.3. Text Feature Extraction")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **Count Vectorizer:**
        - ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói t·ª´
        - V√≠ d·ª•: "I love ML" ‚Üí [1, 1, 1]
        
        **∆Øu ƒëi·ªÉm:**
        - ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu
        - Ph√π h·ª£p v·ªõi short texts
        """)
    
    with col2:
        st.write("""
        **TF-IDF Vectorizer:**
        - TF (Term Frequency): T·∫ßn su·∫•t t·ª´ trong document
        - IDF (Inverse Document Frequency): ƒê·ªô quan tr·ªçng c·ªßa t·ª´
        - TF-IDF = TF √ó IDF
        
        **∆Øu ƒëi·ªÉm:**
        - Ph·∫£n √°nh t·∫ßm quan tr·ªçng c·ªßa t·ª´
        - Gi·∫£m tr·ªçng s·ªë c·ªßa t·ª´ ph·ªï bi·∫øn (the, is, a,...)
        """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Mode selection
    demo_mode = st.radio("Ch·ªçn mode:", ["Nh·∫≠p text t·ª± do", "M·∫´u c√≥ s·∫µn"], horizontal=True)
    
    if demo_mode == "Nh·∫≠p text t·ª± do":
        st.write("**Nh·∫≠p c√°c ƒëo·∫°n text (m·ªói d√≤ng = 1 document):**")
        
        text_input = st.text_area(
            "Your texts:",
            value="I love machine learning\nPython is great for data science\nDeep learning requires GPUs",
            height=150
        )
        
        texts = [t.strip() for t in text_input.split('\n') if t.strip()]
    
    else:
        texts = [
            "Machine learning is a subset of artificial intelligence",
            "Data science combines statistics and programming",
            "Python is the most popular language for machine learning",
            "Deep learning uses neural networks with multiple layers",
            "Natural language processing deals with text and speech"
        ]
        st.write("**Sample texts:**")
        for i, text in enumerate(texts, 1):
            st.write(f"{i}. {text}")
    
    # Parameters
    col1, col2, col3 = st.columns(3)
    with col1:
        method = st.selectbox("Method:", ["tfidf", "count", "hashing"])
    with col2:
        max_features = st.slider("Max features:", 10, 50, 20)
    with col3:
        show_top = st.slider("Show top words:", 5, 20, 10)
    
    if st.button("üöÄ Extract Text Features"):
        extractor = TextFeatureExtractor(method=method, max_features=max_features)
        features, feature_names, top_words = extractor.analyze_text(texts)
        
        st.success("‚úÖ Extraction ho√†n t·∫•t!")
        
        # Feature matrix
        st.subheader("üìä Feature Matrix")
        df_features = pd.DataFrame(features, columns=feature_names)
        df_features.index = [f"Doc {i+1}" for i in range(len(texts))]
        st.dataframe(df_features, use_container_width=True)
        
        st.write(f"üìè **Shape:** {features.shape} ({len(texts)} documents √ó {features.shape[1]} words)")
        
        # Top words
        col_a, col_b = st.columns([1, 2])
        
        with col_a:
            st.subheader(f"üîù Top {show_top} Words")
            top_df = pd.DataFrame(top_words[:show_top], columns=['Word', 'Score'])
            st.dataframe(top_df, use_container_width=True)
        
        with col_b:
            st.subheader("üìà Visualization")
            fig = extractor.visualize_features(texts, feature_names, features)
            st.plotly_chart(fig, use_container_width=True)
        
        # Explanation
        with st.expander("üí° Gi·∫£i th√≠ch k·∫øt qu·∫£"):
            if method == 'tfidf':
                st.write("""
                **TF-IDF scores cao** = t·ª´ quan tr·ªçng trong document v√† √≠t xu·∫•t hi·ªán trong c√°c documents kh√°c
                
                V√≠ d·ª•:
                - "learning", "machine" c√≥ score cao v√¨ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn
                - "the", "is", "a" b·ªã lo·∫°i b·ªè (stopwords) ho·∫∑c c√≥ score th·∫•p
                """)
            else:
                st.write("""
                **Count values** = s·ªë l·∫ßn t·ª´ xu·∫•t hi·ªán trong m·ªói document
                
                - Count = 3: t·ª´ xu·∫•t hi·ªán 3 l·∫ßn
                - Count = 0: t·ª´ kh√¥ng xu·∫•t hi·ªán
                """)

# ==================== IMAGE FEATURES (SCIKIT-LEARN VERSION) ====================
elif page == "üñºÔ∏è Image Features":
    st.header("7.2.4. Image Feature Extraction (Scikit-learn)")
    
    st.info("‚ö†Ô∏è **Theo t√†i li·ªáu scikit-learn**, Image Feature Extraction bao g·ªìm: **Patch Extraction** v√† **Image-to-Graph Conversion**")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **1Ô∏è‚É£ Patch Extraction:**
        - Chia ·∫£nh th√†nh patches nh·ªè
        - H·ªçc local features t·ª´ raw pixels
        - API: `extract_patches_2d()`, `PatchExtractor`
        
        **·ª®ng d·ª•ng:**
        - Dictionary Learning
        - Image Denoising
        - Texture Analysis
        """)
    
    with col2:
        st.write("""
        **2Ô∏è‚É£ Image-to-Graph:**
        - Chuy·ªÉn ·∫£nh th√†nh graph structure
        - M·ªói pixel = 1 node trong graph
        - API: `img_to_graph()`
        
        **·ª®ng d·ª•ng:**
        - Spectral Clustering
        - Image Segmentation
        - Region Analysis
        """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Import sklearn version
    from utils.image_features import ImageFeatureExtractor
    
    # Upload or select image
    uploaded_file = st.file_uploader("Upload ·∫£nh c·ªßa b·∫°n:", type=['png', 'jpg', 'jpeg'])
    
    st.write("**Ho·∫∑c ch·ªçn ·∫£nh m·∫´u:**")
    sample_choice = st.selectbox(
        "Ch·ªçn ·∫£nh m·∫´u:",
        ["Kh√¥ng ch·ªçn", "Red Image", "Green Image", "Pattern Image", "Gradient Image"]
    )
    
    # X·ª≠ l√Ω ·∫£nh
    image = None
    if sample_choice != "Kh√¥ng ch·ªçn":
        sample_map = {
            "Red Image": "datasets/sample_images/red_image.png",
            "Green Image": "datasets/sample_images/green_image.png",
            "Pattern Image": "datasets/sample_images/pattern_image.png",
            "Gradient Image": "datasets/sample_images/gradient_image.png"
        }
        
        sample_path = sample_map[sample_choice]
        if os.path.exists(sample_path):
            image = Image.open(sample_path)
        else:
            st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh m·∫´u: {sample_path}")
    
    elif uploaded_file is not None:
        image = Image.open(uploaded_file)
    
    # Method selection & parameters
    col1, col2 = st.columns([1, 2])
    
    with col1:
        method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p:", ["patches", "graph"])
        
        if method == "patches":
            st.write("**Tham s·ªë Patch Extraction:**")
            patch_size = st.slider("Patch size:", 16, 64, 32, 8)
            max_patches = st.slider("Max patches:", 20, 200, 100, 10)
        else:
            st.write("**Tham s·ªë Graph Clustering:**")
            n_clusters = st.slider("Number of clusters:", 2, 8, 3)
    
    if image is not None:
        with col2:
            st.write("**Original Image:**")
            st.image(image, width=300)
            image_array = np.array(image)
            st.caption(f"Size: {image_array.shape[1]}√ó{image_array.shape[0]} pixels")
        
        if st.button("üöÄ Extract Features (scikit-learn)"):
            if method == "patches":
                # ========== PATCH EXTRACTION ==========
                extractor = ImageFeatureExtractor(method='patches')
                
                with st.spinner("Extracting patches..."):
                    features, extras = extractor.extract_features(
                        image,
                        patch_size=(patch_size, patch_size),
                        max_patches=max_patches
                    )
                    
                    st.success("‚úÖ Patch Extraction ho√†n t·∫•t!")
                    
                    # Show info
                    st.subheader("üìä K·∫øt Qu·∫£ Patch Extraction")
                    
                    # Metrics
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Number of Patches", extras['n_patches'])
                    with col_b:
                        st.metric("Patch Size", f"{patch_size}√ó{patch_size}")
                    with col_c:
                        st.metric("Features per Patch", patch_size * patch_size * 3)
                    
                    # Details
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write("**üìã Th√¥ng tin chi ti·∫øt:**")
                        st.write(f"- Patches shape: `{extras['patches'].shape}`")
                        st.write(f"- Flattened features: `{features.shape}`")
                        st.write(f"- Total features: `{features.shape[0] * features.shape[1]:,}`")
                        
                        st.write("\n**üî¢ First patch (first 20 features):**")
                        st.write(features[0][:20])
                        
                        # Explain patch calculation
                        with st.expander("üí° C√°ch t√≠nh s·ªë patches"):
                            st.write(f"""
                            **Ph∆∞∆°ng ph√°p: Random Sampling**
                            
                            - ·∫¢nh g·ªëc: {image_array.shape[1]}√ó{image_array.shape[0]}
                            - Patch size: {patch_size}√ó{patch_size}
                            - `max_patches={max_patches}` ‚Üí Random sampling {max_patches} v·ªã tr√≠
                            
                            **N·∫øu c·∫Øt ƒë·ªÅu (non-overlapping):**
                            - S·ªë patches ngang: {image_array.shape[1] // patch_size}
                            - S·ªë patches d·ªçc: {image_array.shape[0] // patch_size}
                            - T·ªïng: {(image_array.shape[1] // patch_size) * (image_array.shape[0] // patch_size)} patches
                            
                            **V·ªõi random sampling:**
                            - Ch·ªçn ng·∫´u nhi√™n {max_patches} v·ªã tr√≠
                            - C√≥ th·ªÉ overlap (ch·ªìng l·∫•n)
                            - Linh ho·∫°t h∆°n cho Dictionary Learning
                            """)
                    
                    with col_b:
                        st.write("**üñºÔ∏è Visualization (16 patches ƒë·∫ßu ti√™n):**")
                        fig = extractor.visualize_patches(extras['patches'], n_display=16)
                        st.pyplot(fig)
                    
                    # Explanation
                    with st.expander("üìñ Gi·∫£i th√≠ch Patch Extraction"):
                        st.write("""
                        **Patch Extraction l√† g√¨?**
                        
                        Chia ·∫£nh l·ªõn th√†nh nhi·ªÅu patches (m·∫£nh) nh·ªè ƒë·ªÉ h·ªçc c√°c local patterns.
                        
                        **Quy tr√¨nh:**
                        1. Ch·ªçn patch size (v√≠ d·ª•: 32√ó32)
                        2. Random sampling ho·∫∑c sliding window
                        3. Extract t·ª´ng patch th√†nh vector
                        4. Flatten: 32√ó32√ó3 = 3,072 features/patch
                        
                        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
                        - **Dictionary Learning**: H·ªçc "alphabet" c·ªßa h√¨nh ·∫£nh
                        - **Image Denoising**: Kh·ª≠ nhi·ªÖu b·∫±ng c√°ch so s√°nh patches
                        - **Texture Recognition**: Ph√¢n lo·∫°i textures (g·ªó, v·∫£i, ƒë√°...)
                        - **Feature Extraction**: D√πng l√†m input cho ML models
                        
                        **Tham kh·∫£o:**
                        - [sklearn.feature_extraction.image.extract_patches_2d](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html)
                        """)
            
            else:
                # ========== IMAGE-TO-GRAPH ==========
                extractor = ImageFeatureExtractor(method='graph')
                
                with st.spinner("Converting to graph & clustering..."):
                    labels, extras = extractor.extract_features(
                        image,
                        n_clusters=n_clusters
                    )
                    
                    st.success("‚úÖ Image-to-Graph Conversion ho√†n t·∫•t!")
                    
                    # Show info
                    st.subheader("üìä K·∫øt Qu·∫£ Graph-based Segmentation")
                    
                    # Metrics
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Number of Nodes", extras['graph'].shape[0])
                    with col_b:
                        st.metric("Number of Clusters", n_clusters)
                    with col_c:
                        st.metric("Graph Edges", extras['graph'].nnz // 2)
                    
                    # Details
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write("**üìã Th√¥ng tin Graph:**")
                        st.write(f"- Graph shape: `{extras['graph'].shape}`")
                        st.write(f"- Graph type: Sparse adjacency matrix")
                        st.write(f"- Labels shape: `{labels.shape}`")
                        st.write(f"- Segmented shape: `{extras['segmented'].shape}`")
                        
                        st.write("\n**üìä Cluster Distribution:**")
                        unique, counts = np.unique(labels, return_counts=True)
                        cluster_data = []
                        for cluster, count in zip(unique, counts):
                            percentage = (count / len(labels)) * 100
                            cluster_data.append({
                                'Cluster': cluster,
                                'Pixels': count,
                                'Percentage': f"{percentage:.1f}%"
                            })
                        st.dataframe(pd.DataFrame(cluster_data), hide_index=True, use_container_width=True)
                        
                        with st.expander("üí° T·∫°i sao resize v·ªÅ 50√ó50?"):
                            st.write("""
                            Graph-based clustering **r·∫•t ch·∫≠m** v·ªõi ·∫£nh l·ªõn v√¨:
                            - ·∫¢nh 256√ó256 = 65,536 nodes ‚Üí Ma tr·∫≠n 65,536 √ó 65,536!
                            - Spectral clustering complexity: O(n¬≥)
                            
                            Resize v·ªÅ 50√ó50:
                            - 2,500 nodes ‚Üí Nhanh h∆°n nhi·ªÅu
                            - V·∫´n gi·ªØ ƒë∆∞·ª£c structure ch√≠nh c·ªßa ·∫£nh
                            - Ph√π h·ª£p cho demo & education
                            """)
                    
                    with col_b:
                        st.write("**üñºÔ∏è Segmentation Result:**")
                        fig = extractor.visualize_segmentation(
                            image_array,
                            extras['segmented'],
                            extras['small_image']
                        )
                        st.pyplot(fig)
                    
                    # Explanation
                    with st.expander("üìñ Gi·∫£i th√≠ch Image-to-Graph"):
                        st.write("""
                        **Image-to-Graph l√† g√¨?**
                        
                        Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh th√†nh c·∫•u tr√∫c ƒë·ªì th·ªã ƒë·ªÉ ph√¢n t√≠ch relationships gi·ªØa c√°c pixels.
                        
                        **C·∫•u tr√∫c Graph:**
                        - **Nodes**: M·ªói pixel = 1 node
                        - **Edges**: K·∫øt n·ªëi v·ªõi 4 ho·∫∑c 8 neighbors
                        - **Weights**: D·ª±a tr√™n ƒë·ªô kh√°c bi·ªát m√†u s·∫Øc
                        
                        **Spectral Clustering:**
                        1. Build graph t·ª´ ·∫£nh
                        2. T√≠nh eigenvectors c·ªßa Laplacian matrix
                        3. Clustering trong eigenspace
                        4. G√°n labels v·ªÅ pixels
                        
                        **·ª®ng d·ª•ng th·ª±c t·∫ø:**
                        - **Medical Imaging**: Ph√¢n v√πng c∆° quan, tumor
                        - **Image Segmentation**: T√°ch object kh·ªèi background
                        - **Region Analysis**: Ph√¢n t√≠ch t·ª´ng v√πng ri√™ng bi·ªát
                        - **Interactive Selection**: Click ƒë·ªÉ select region
                        
                        **∆Øu ƒëi·ªÉm:**
                        - Unsupervised (kh√¥ng c·∫ßn labels)
                        - T·ª± ƒë·ªông t√¨m boundaries
                        - Consider c·∫£ color v√† spatial proximity
                        
                        **Tham kh·∫£o:**
                        - [sklearn.feature_extraction.image.img_to_graph](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html)
                        - [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)
                        """)
    
    else:
        st.info("üëÜ Upload m·ªôt ·∫£nh ho·∫∑c ch·ªçn ·∫£nh m·∫´u ƒë·ªÉ b·∫Øt ƒë·∫ßu!")
        
        # Documentation links
        st.markdown("---")
        st.write("**üìö T√†i li·ªáu tham kh·∫£o ch√≠nh th·ª©c:**")
        st.markdown("""
        - [Scikit-learn Feature Extraction Documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction)
        - [`sklearn.feature_extraction.image.extract_patches_2d`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html)
        - [`sklearn.feature_extraction.image.PatchExtractor`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.PatchExtractor.html)
        - [`sklearn.feature_extraction.image.img_to_graph`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html)
        - [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)
        """)
        
        st.write("**üí° L∆∞u √Ω:**")
        st.info("""
        ƒê√¢y l√† Image Feature Extraction theo **t√†i li·ªáu scikit-learn ch√≠nh th·ª©c**, 
        kh√°c v·ªõi Computer Vision truy·ªÅn th·ªëng (Color Histogram, HOG, SIFT, CNN features).
        """)

# ==================== SO S√ÅNH ====================
elif page == "üî¨ So s√°nh":
    st.header("So s√°nh 4 Ph∆∞∆°ng ph√°p Feature Extraction")
    
    st.subheader("üìä B·∫£ng so s√°nh t·ªïng quan")
    
    comparison_data = {
        'Ph∆∞∆°ng ph√°p': ['Dict Features', 'Feature Hashing', 'Text Features', 'Image Features'],
        'Input Type': ['Dictionary', 'Dictionary', 'Text', 'Image'],
        'Output Type': ['Dense Vector', 'Dense Vector', 'Sparse Vector', 'Dense Vector'],
        'S·ªë Features': ['T·ª± ƒë·ªông (= unique values)', 'C·ªë ƒë·ªãnh (user ƒë·ªãnh)', 'User ƒë·ªãnh (max_features)', 'Ph·ª• thu·ªôc method'],
        'T·ªëc ƒë·ªô': ['Nhanh', 'R·∫•t nhanh ‚ö°', 'Trung b√¨nh', 'Ch·∫≠m'],
        'B·ªô nh·ªõ': ['Trung b√¨nh', 'Th·∫•p üíæ', 'Cao', 'R·∫•t cao'],
        '·ª®ng d·ª•ng': ['Categorical data', 'Large vocabulary', 'NLP, Text Mining', 'Computer Vision']
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    st.dataframe(df_comparison, use_container_width=True)
    
    st.markdown("---")
    
    # ∆Øu nh∆∞·ª£c ƒëi·ªÉm
    st.subheader("‚öñÔ∏è ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**1Ô∏è‚É£ Dict Features (DictVectorizer)**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - D·ªÖ hi·ªÉu, interpretable
        - Gi·ªØ nguy√™n feature names
        - Kh√¥ng b·ªã hash collision
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - T·ªën b·ªô nh·ªõ v·ªõi large vocabulary
        - C·∫ßn fit tr∆∞·ªõc khi transform
        - Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c unseen values
        """)
        
        st.write("**2Ô∏è‚É£ Feature Hashing**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - R·∫•t nhanh, scalable
        - Kh√¥ng c·∫ßn l∆∞u vocabulary
        - X·ª≠ l√Ω ƒë∆∞·ª£c unseen values
        - Ti·∫øt ki·ªám b·ªô nh·ªõ
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - Hash collision
        - M·∫•t interpretability
        - Kh√¥ng th·ªÉ reverse
        """)
    
    with col2:
        st.write("**3Ô∏è‚É£ Text Features (TF-IDF)**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - Ph·∫£n √°nh importance c·ªßa t·ª´
        - Gi·∫£m noise (stopwords)
        - Hi·ªáu qu·∫£ cho text classification
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - M·∫•t th·ª© t·ª± t·ª´ (bag of words)
        - Kh√¥ng hi·ªÉu ng·ªØ nghƒ©a
        - Sparse vector (t·ªën memory)
        """)
        
        st.write("**4Ô∏è‚É£ Image Features**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - Capture visual information
        - Robust v·ªõi transformations
        - Nhi·ªÅu methods l·ª±a ch·ªçn
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - T√≠nh to√°n ch·∫≠m
        - High dimensional
        - C·∫ßn preprocessing
        """)
    
    st.markdown("---")
    
    # Performance comparison
    st.subheader("‚ö° So s√°nh Performance")
    
    perf_data = {
        'Method': ['Dict', 'Hash', 'Text (TF-IDF)', 'Image (HOG)'],
        'Training Time': [0.01, 0.005, 0.5, 2.0],
        'Inference Time': [0.005, 0.002, 0.1, 1.5],
        'Memory Usage (MB)': [10, 5, 50, 100]
    }
    
    df_perf = pd.DataFrame(perf_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Training Time (seconds):**")
        st.bar_chart(df_perf.set_index('Method')['Training Time'])
    
    with col2:
        st.write("**Memory Usage (MB):**")
        st.bar_chart(df_perf.set_index('Method')['Memory Usage (MB)'])
    
    st.caption("*S·ªë li·ªáu mang t√≠nh ch·∫•t minh h·ªça v·ªõi dataset nh·ªè")
    
    st.markdown("---")
    
    # Khi n√†o d√πng c√°i g√¨
    st.subheader("üéØ Khi n√†o n√™n d√πng ph∆∞∆°ng ph√°p n√†o?")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **D√πng Dict Features khi:**
        - Dataset nh·ªè, s·ªë features kh√¥ng qu√° l·ªõn
        - C·∫ßn interpretability
        - C√≥ categorical data
        - V√≠ d·ª•: D·ªØ li·ªáu kh√°ch h√†ng, s·∫£n ph·∫©m
        """)
        
        st.info("""
        **D√πng Feature Hashing khi:**
        - Dataset r·∫•t l·ªõn (millions of features)
        - C·∫ßn speed & scalability
        - Kh√¥ng quan t√¢m interpretability
        - V√≠ d·ª•: Click-through rate prediction
        """)
    
    with col2:
        st.info("""
        **D√πng Text Features khi:**
        - L√†m vi·ªác v·ªõi vƒÉn b·∫£n
        - Text classification, sentiment analysis
        - C√≥ ƒë·ªß b·ªô nh·ªõ
        - V√≠ d·ª•: Spam detection, topic modeling
        """)
        
        st.info("""
        **D√πng Image Features khi:**
        - L√†m vi·ªác v·ªõi ·∫£nh
        - Computer vision tasks
        - C√≥ GPU (cho deep learning)
        - V√≠ d·ª•: Object detection, face recognition
        """)
    
    st.markdown("---")
    
    # Summary
    st.subheader("üìù T√≥m t·∫Øt")
    st.write("""
    **Kh√¥ng c√≥ ph∆∞∆°ng ph√°p n√†o l√† t·ªët nh·∫•t cho m·ªçi tr∆∞·ªùng h·ª£p!**
    
    L·ª±a ch·ªçn ph∆∞∆°ng ph√°p ph·ª• thu·ªôc v√†o:
    1. **Lo·∫°i d·ªØ li·ªáu:** Categorical, Text, Image?
    2. **K√≠ch th∆∞·ªõc dataset:** L·ªõn hay nh·ªè?
    3. **Y√™u c·∫ßu:** Speed, Accuracy, Interpretability?
    4. **Resources:** Memory, CPU, GPU?
    
    üí° **Best practice:** Th·ª≠ nhi·ªÅu methods v√† so s√°nh k·∫øt qu·∫£!
    """)

# ==================== ML PERFORMANCE COMPARISON ====================
elif page == "üéØ ML Performance":
    st.header("üéØ So s√°nh Performance v·ªõi Machine Learning")
    
    st.write("""
    Trang n√†y demo **hi·ªáu su·∫•t th·ª±c t·∫ø** c·ªßa c√°c ph∆∞∆°ng ph√°p feature extraction 
    khi k·∫øt h·ª£p v·ªõi Machine Learning models.
    """)
    
    st.markdown("---")
    
    # Demo 1: Dict vs Hash v·ªõi Titanic
    st.subheader("1Ô∏è‚É£ Dict Features vs Feature Hashing (Titanic Dataset)")
    
    if st.button("üöÄ Ch·∫°y so s√°nh Dict vs Hash"):
        with st.spinner("ƒêang training models..."):
            try:
                df = pd.read_csv('datasets/titanic.csv')
                
                # Chu·∫©n b·ªã data
                df_clean = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']].dropna()
                df_clean['Age'] = df_clean['Age'].astype(int)
                df_clean['Fare'] = df_clean['Fare'].astype(int)
                
                # Convert to dict
                dict_data = df_clean[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].to_dict('records')
                labels = df_clean['Survived'].values
                
                # Import comparison
                from utils.model_comparison import FeatureExtractionComparison
                comparator = FeatureExtractionComparison()
                
                results = comparator.compare_dict_vs_hash(dict_data, labels)
                
                st.success("‚úÖ Ho√†n t·∫•t!")
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("DictVectorizer Accuracy", f"{results['DictVectorizer']['accuracy']:.2%}")
                    st.write(f"‚è±Ô∏è Transform time: {results['DictVectorizer']['transform_time']:.4f}s")
                    st.write(f"üèãÔ∏è Train time: {results['DictVectorizer']['train_time']:.4f}s")
                    st.write(f"üìä Features: {results['DictVectorizer']['n_features']}")
                    st.write(f"üíæ Memory: {results['DictVectorizer']['memory_mb']:.2f} MB")
                
                with col2:
                    st.metric("FeatureHasher Accuracy", f"{results['FeatureHasher']['accuracy']:.2%}")
                    st.write(f"‚è±Ô∏è Transform time: {results['FeatureHasher']['transform_time']:.4f}s")
                    st.write(f"üèãÔ∏è Train time: {results['FeatureHasher']['train_time']:.4f}s")
                    st.write(f"üìä Features: {results['FeatureHasher']['n_features']}")
                    st.write(f"üíæ Memory: {results['FeatureHasher']['memory_mb']:.2f} MB")
                
                # Chart
                import plotly.graph_objects as go
                
                fig = go.Figure(data=[
                    go.Bar(name='DictVectorizer', x=['Accuracy', 'Speed (1/time)', 'Memory Efficiency'], 
                           y=[results['DictVectorizer']['accuracy'], 
                              1/results['DictVectorizer']['transform_time'],
                              1/results['DictVectorizer']['memory_mb']]),
                    go.Bar(name='FeatureHasher', x=['Accuracy', 'Speed (1/time)', 'Memory Efficiency'], 
                           y=[results['FeatureHasher']['accuracy'],
                              1/results['FeatureHasher']['transform_time'],
                              1/results['FeatureHasher']['memory_mb']])
                ])
                
                fig.update_layout(barmode='group', title='Performance Comparison')
                st.plotly_chart(fig, use_container_width=True)
                
                st.info("""
                **üí° Nh·∫≠n x√©t:**
                - DictVectorizer th∆∞·ªùng c√≥ accuracy cao h∆°n (nhi·ªÅu features h∆°n)
                - FeatureHasher nhanh h∆°n v√† ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n
                - Trade-off: Accuracy vs Speed/Memory
                """)
                
            except FileNotFoundError:
                st.error("‚ùå Kh√¥ng t√¨m th·∫•y datasets/titanic.csv")
            except Exception as e:
                st.error(f"‚ùå L·ªói: {str(e)}")
    
    st.markdown("---")
    
    # Demo 2: Count vs TF-IDF
    st.subheader("2Ô∏è‚É£ Count vs TF-IDF (Text Classification)")
    
    sample_texts = [
        "I love this product, it's amazing!",
        "Terrible quality, waste of money",
        "Best purchase ever, highly recommend",
        "Poor service, very disappointed",
        "Excellent value for money",
        "Not worth it, very bad",
        "Great experience, will buy again",
        "Horrible, do not buy",
        "Perfect, exactly what I needed",
        "Worst product ever"
    ]
    
    sample_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative
    
    st.write("**Sample data (Sentiment Analysis):**")
    for i, (text, label) in enumerate(zip(sample_texts[:5], sample_labels[:5])):
        st.write(f"{i+1}. [{'+' if label else '-'}] {text}")
    
    if st.button("üöÄ Ch·∫°y so s√°nh Text Methods"):
        with st.spinner("ƒêang training..."):
            # T·∫°o th√™m data ƒë·ªÉ c√≥ ƒë·ªß cho train/test split
            texts = sample_texts * 20  # 200 samples
            labels_expanded = sample_labels * 20
            
            from utils.model_comparison import FeatureExtractionComparison
            comparator = FeatureExtractionComparison()
            
            results = comparator.compare_text_methods(texts, labels_expanded)
            
            st.success("‚úÖ Ho√†n t·∫•t!")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("CountVectorizer Accuracy", f"{results['CountVectorizer']['accuracy']:.2%}")
                st.write(f"‚è±Ô∏è Time: {results['CountVectorizer']['transform_time']:.4f}s")
                st.write(f"üìä Features: {results['CountVectorizer']['n_features']}")
            
            with col2:
                st.metric("TfidfVectorizer Accuracy", f"{results['TfidfVectorizer']['accuracy']:.2%}")
                st.write(f"‚è±Ô∏è Time: {results['TfidfVectorizer']['transform_time']:.4f}s")
                st.write(f"üìä Features: {results['TfidfVectorizer']['n_features']}")
            
            st.info("""
            **üí° Nh·∫≠n x√©t:**
            - TF-IDF th∆∞·ªùng perform t·ªët h∆°n cho text classification
            - CountVectorizer ƒë∆°n gi·∫£n h∆°n, ph√π h·ª£p v·ªõi short texts
            """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>üéì Demo by Group 8 - M√¥n Khai Ph√° D·ªØ Li·ªáu</p>
    <p>Made with ‚ù§Ô∏è using Streamlit</p>
</div>
""", unsafe_allow_html=True)
