"""
WEB APP FEATURE EXTRACTION DEMO
Streamlit application t√≠ch h·ª£p 4 ph∆∞∆°ng ph√°p feature extraction
"""

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import sys
import os

# Add utils to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from dict_features import DictFeatureExtractor
from hash_features import HashFeatureExtractor
from text_features import TextFeatureExtractor
from image_features import ImageFeatureExtractor

# Page config
st.set_page_config(
    page_title="Feature Extraction Demo",
    page_icon="üöÄ",
    layout="wide"
)

# Title
st.title("üöÄ Feature Extraction Demo")
st.markdown("**Nh√≥m X - M√¥n Khai Ph√° D·ªØ Li·ªáu**")
st.markdown("---")

# Sidebar
st.sidebar.title("üìã Navigation")
page = st.sidebar.radio(
    "Ch·ªçn ph∆∞∆°ng ph√°p:",
    ["üè† T·ªïng quan", "üìä Dict Features", "# Feature Hashing", "üìù Text Features", "üñºÔ∏è Image Features", "üî¨ So s√°nh"]
)

# ==================== TRANG T·ªîNG QUAN ====================
if page == "üè† T·ªïng quan":
    st.header("Gi·ªõi thi·ªáu Feature Extraction")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìñ Feature Extraction l√† g√¨?")
        st.write("""
        Feature Extraction (Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng) l√† qu√° tr√¨nh chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ 
        (raw data) th√†nh d·∫°ng s·ªë (numerical) m√† c√°c thu·∫≠t to√°n machine learning c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c.
        
        **T·∫°i sao c·∫ßn Feature Extraction?**
        - Machine learning ch·ªâ l√†m vi·ªác v·ªõi s·ªë
        - Gi·∫£m chi·ªÅu d·ªØ li·ªáu (dimensionality reduction)
        - Gi·ªØ l·∫°i th√¥ng tin quan tr·ªçng
        - C·∫£i thi·ªán hi·ªáu su·∫•t model
        """)
    
    with col2:
        st.subheader("üéØ 4 Ph∆∞∆°ng ph√°p trong Demo")
        st.write("""
        1. **Dict Features**: Chuy·ªÉn dictionary ‚Üí vector
        2. **Feature Hashing**: Hash trick cho high-dimensional data
        3. **Text Features**: Chuy·ªÉn text ‚Üí vector (TF-IDF, Count)
        4. **Image Features**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng t·ª´ ·∫£nh
        """)
    
    st.markdown("---")
    st.subheader("üìä Quy tr√¨nh chung")
    st.image("https://miro.medium.com/max/1400/1*VQvV5kVXZvHlmBDxWcZvNg.png", 
             caption="Feature Extraction Pipeline", use_column_width=True)
    
    st.info("üëà Ch·ªçn ph∆∞∆°ng ph√°p ·ªü sidebar ƒë·ªÉ b·∫Øt ƒë·∫ßu demo!")

# ==================== DICT FEATURES ====================
elif page == "üìä Dict Features":
    st.header("7.2.1. Loading Features from Dicts")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    st.write("""
    **DictVectorizer** chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu d·∫°ng dictionary th√†nh feature vectors.
    
    **V√≠ d·ª•:**
```python
    Input:  [{'city': 'Hanoi', 'age': 25}, {'city': 'HCM', 'age': 30}]
    Output: [[25, 0, 1], [30, 1, 0]]  # [age, city=HCM, city=Hanoi]
```
    
    **·ª®ng d·ª•ng:** D·ªØ li·ªáu categorical nh∆∞ th√¥ng tin kh√°ch h√†ng, s·∫£n ph·∫©m, v.v.
    """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Ch·ªçn mode
    demo_mode = st.radio("Ch·ªçn mode:", ["V√≠ d·ª• ƒë∆°n gi·∫£n", "Titanic Dataset"])
    
    if demo_mode == "V√≠ d·ª• ƒë∆°n gi·∫£n":
        st.write("**Nh·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n:**")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            name1 = st.text_input("T√™n 1:", "An")
            city1 = st.selectbox("Th√†nh ph·ªë 1:", ["Hanoi", "HCM", "Danang"], key="city1")
            age1 = st.number_input("Tu·ªïi 1:", 18, 100, 25)
        
        with col2:
            name2 = st.text_input("T√™n 2:", "Binh")
            city2 = st.selectbox("Th√†nh ph·ªë 2:", ["Hanoi", "HCM", "Danang"], key="city2")
            age2 = st.number_input("Tu·ªïi 2:", 18, 100, 30)
        
        with col3:
            name3 = st.text_input("T√™n 3:", "Chi")
            city3 = st.selectbox("Th√†nh ph·ªë 3:", ["Hanoi", "HCM", "Danang"], key="city3")
            age3 = st.number_input("Tu·ªïi 3:", 18, 100, 22)
        
        if st.button("üöÄ Extract Features"):
            data = [
                {'name': name1, 'city': city1, 'age': age1},
                {'name': name2, 'city': city2, 'age': age2},
                {'name': name3, 'city': city3, 'age': age3}
            ]
            
            extractor = DictFeatureExtractor()
            features, feature_names = extractor.extract_features(data)
            
            st.success("‚úÖ Extraction ho√†n t·∫•t!")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.write("**Input Data:**")
                st.json(data)
            
            with col_b:
                st.write("**Feature Names:**")
                st.write(list(feature_names))
            
            st.write("**Feature Matrix:**")
            df_features = pd.DataFrame(features, columns=feature_names)
            st.dataframe(df_features, use_container_width=True)
            
            st.write(f"üìè **Shape:** {features.shape} (3 samples √ó {features.shape[1]} features)")
    
    else:  # Titanic Dataset
        st.write("**Demo v·ªõi Titanic Dataset:**")
        
        try:
            df = pd.read_csv('datasets/titanic.csv')
            st.write("Dataset preview:")
            st.dataframe(df.head(), use_container_width=True)
            
            n_samples = st.slider("S·ªë l∆∞·ª£ng samples:", 5, 50, 10)
            
            if st.button("üöÄ Extract Features from Titanic"):
                extractor = DictFeatureExtractor()
                dict_data, features, feature_names = extractor.demo_with_titanic(df, n_samples)
                
                st.success(f"‚úÖ ƒê√£ extract {n_samples} samples!")
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.write("**Original Data (dict format):**")
                    st.json(dict_data[:3])  # Show first 3
                
                with col_b:
                    st.write("**Feature Names:**")
                    st.write(list(feature_names))
                
                st.write("**Feature Matrix:**")
                df_features = pd.DataFrame(features, columns=feature_names)
                st.dataframe(df_features, use_container_width=True)
                
                st.write(f"üìè **Shape:** {features.shape}")
        
        except FileNotFoundError:
            st.error("‚ùå Kh√¥ng t√¨m th·∫•y datasets/titanic.csv. Vui l√≤ng ch·∫°y download_data.py tr∆∞·ªõc!")

# ==================== FEATURE HASHING ====================
elif page == "# Feature Hashing":
    st.header("7.2.2. Feature Hashing")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    st.write("""
    **Feature Hashing** (Hashing Trick) s·ª≠ d·ª•ng hash function ƒë·ªÉ chuy·ªÉn features th√†nh vector 
    c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh.
    
    **∆Øu ƒëi·ªÉm:**
    - ‚ö° R·∫•t nhanh (kh√¥ng c·∫ßn l∆∞u vocabulary)
    - üíæ Ti·∫øt ki·ªám b·ªô nh·ªõ
    - üîÑ X·ª≠ l√Ω ƒë∆∞·ª£c unseen features
    
    **Nh∆∞·ª£c ƒëi·ªÉm:**
    - ‚ö†Ô∏è Hash collision (nhi·ªÅu features ‚Üí c√πng 1 hash value)
    - ‚ùì Kh√¥ng bi·∫øt feature g·ªëc l√† g√¨ (one-way)
    """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        country1 = st.text_input("Country 1:", "Vietnam")
        city1 = st.text_input("City 1:", "Hanoi")
        age1 = st.number_input("Age 1:", 18, 100, 25, key="hash_age1")
    
    with col2:
        country2 = st.text_input("Country 2:", "Thailand")
        city2 = st.text_input("City 2:", "Bangkok")
        age2 = st.number_input("Age 2:", 18, 100, 28, key="hash_age2")
    
    with col3:
        country3 = st.text_input("Country 3:", "Vietnam")
        city3 = st.text_input("City 3:", "HCM")
        age3 = st.number_input("Age 3:", 18, 100, 30, key="hash_age3")
    
    n_features = st.slider("S·ªë features sau khi hash:", 5, 20, 10)
    
    if st.button("üöÄ Extract & Compare"):
        data = [
            {'country': country1, 'city': city1, 'age': age1},
            {'country': country2, 'city': city2, 'age': age2},
            {'country': country3, 'city': city3, 'age': age3}
        ]
        
        extractor = HashFeatureExtractor(n_features=n_features)
        comparison, dict_features, hash_features = extractor.compare_with_dict_vectorizer(data)
        
        st.success("‚úÖ Extraction ho√†n t·∫•t!")
        
        # Show original data
        st.write("**Input Data:**")
        st.json(data)
        
        # Comparison table
        st.subheader("üìä So s√°nh DictVectorizer vs FeatureHasher")
        
        comp_df = pd.DataFrame({
            'Method': ['DictVectorizer', 'FeatureHasher'],
            'Number of Features': [comparison['DictVectorizer']['n_features'], 
                                   comparison['FeatureHasher']['n_features']],
            'Memory (bytes)': [comparison['DictVectorizer']['memory_size'],
                              comparison['FeatureHasher']['memory_size']]
        })
        st.dataframe(comp_df, use_container_width=True)
        
        # Show features
        col_a, col_b = st.columns(2)
        
        with col_a:
            st.write("**DictVectorizer Output:**")
            st.write(dict_features)
            st.caption(f"Shape: {dict_features.shape}")
        
        with col_b:
            st.write("**FeatureHasher Output:**")
            st.write(hash_features)
            st.caption(f"Shape: {hash_features.shape}")
        
        st.info("""
        **üí° Nh·∫≠n x√©t:**
        - FeatureHasher c√≥ s·ªë features c·ªë ƒë·ªãnh (b·∫°n ch·ªçn)
        - DictVectorizer c√≥ s·ªë features = s·ªë unique values
        - FeatureHasher ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n khi c√≥ nhi·ªÅu categorical values
        """)

# ==================== TEXT FEATURES ====================
elif page == "üìù Text Features":
    st.header("7.2.3. Text Feature Extraction")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **Count Vectorizer:**
        - ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa m·ªói t·ª´
        - V√≠ d·ª•: "I love ML" ‚Üí [1, 1, 1]
        
        **∆Øu ƒëi·ªÉm:**
        - ƒê∆°n gi·∫£n, d·ªÖ hi·ªÉu
        - Ph√π h·ª£p v·ªõi short texts
        """)
    
    with col2:
        st.write("""
        **TF-IDF Vectorizer:**
        - TF (Term Frequency): T·∫ßn su·∫•t t·ª´ trong document
        - IDF (Inverse Document Frequency): ƒê·ªô quan tr·ªçng c·ªßa t·ª´
        - TF-IDF = TF √ó IDF
        
        **∆Øu ƒëi·ªÉm:**
        - Ph·∫£n √°nh t·∫ßm quan tr·ªçng c·ªßa t·ª´
        - Gi·∫£m tr·ªçng s·ªë c·ªßa t·ª´ ph·ªï bi·∫øn (the, is, a,...)
        """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Mode selection
    demo_mode = st.radio("Ch·ªçn mode:", ["Nh·∫≠p text t·ª± do", "M·∫´u c√≥ s·∫µn"], horizontal=True)
    
    if demo_mode == "Nh·∫≠p text t·ª± do":
        st.write("**Nh·∫≠p c√°c ƒëo·∫°n text (m·ªói d√≤ng = 1 document):**")
        
        text_input = st.text_area(
            "Your texts:",
            value="I love machine learning\nPython is great for data science\nDeep learning requires GPUs",
            height=150
        )
        
        texts = [t.strip() for t in text_input.split('\n') if t.strip()]
    
    else:
        texts = [
            "Machine learning is a subset of artificial intelligence",
            "Data science combines statistics and programming",
            "Python is the most popular language for machine learning",
            "Deep learning uses neural networks with multiple layers",
            "Natural language processing deals with text and speech"
        ]
        st.write("**Sample texts:**")
        for i, text in enumerate(texts, 1):
            st.write(f"{i}. {text}")
    
    # Parameters
    col1, col2, col3 = st.columns(3)
    with col1:
        method = st.selectbox("Method:", ["tfidf", "count"])
    with col2:
        max_features = st.slider("Max features:", 10, 50, 20)
    with col3:
        show_top = st.slider("Show top words:", 5, 20, 10)
    
    if st.button("üöÄ Extract Text Features"):
        extractor = TextFeatureExtractor(method=method, max_features=max_features)
        features, feature_names, top_words = extractor.analyze_text(texts)
        
        st.success("‚úÖ Extraction ho√†n t·∫•t!")
        
        # Feature matrix
        st.subheader("üìä Feature Matrix")
        df_features = pd.DataFrame(features, columns=feature_names)
        df_features.index = [f"Doc {i+1}" for i in range(len(texts))]
        st.dataframe(df_features, use_container_width=True)
        
        st.write(f"üìè **Shape:** {features.shape} ({len(texts)} documents √ó {features.shape[1]} words)")
        
        # Top words
        col_a, col_b = st.columns([1, 2])
        
        with col_a:
            st.subheader(f"üîù Top {show_top} Words")
            top_df = pd.DataFrame(top_words[:show_top], columns=['Word', 'Score'])
            st.dataframe(top_df, use_container_width=True)
        
        with col_b:
            st.subheader("üìà Visualization")
            fig = extractor.visualize_features(texts, feature_names, features)
            st.plotly_chart(fig, use_container_width=True)
        
        # Explanation
        with st.expander("üí° Gi·∫£i th√≠ch k·∫øt qu·∫£"):
            if method == 'tfidf':
                st.write("""
                **TF-IDF scores cao** = t·ª´ quan tr·ªçng trong document v√† √≠t xu·∫•t hi·ªán trong c√°c documents kh√°c
                
                V√≠ d·ª•:
                - "learning", "machine" c√≥ score cao v√¨ xu·∫•t hi·ªán nhi·ªÅu l·∫ßn
                - "the", "is", "a" b·ªã lo·∫°i b·ªè (stopwords) ho·∫∑c c√≥ score th·∫•p
                """)
            else:
                st.write("""
                **Count values** = s·ªë l·∫ßn t·ª´ xu·∫•t hi·ªán trong m·ªói document
                
                - Count = 3: t·ª´ xu·∫•t hi·ªán 3 l·∫ßn
                - Count = 0: t·ª´ kh√¥ng xu·∫•t hi·ªán
                """)

# ==================== IMAGE FEATURES ====================
elif page == "üñºÔ∏è Image Features":
    st.header("7.2.4. Image Feature Extraction")
    
    st.subheader("üìö L√Ω thuy·∫øt")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("""
        **Color Histogram:**
        - ƒê·∫øm s·ªë l∆∞·ª£ng pixels cho m·ªói m√†u
        - 3 histograms: Red, Green, Blue
        
        **·ª®ng d·ª•ng:**
        - Image similarity
        - Object tracking
        """)
    
    with col2:
        st.write("""
        **HOG (Histogram of Oriented Gradients):**
        - Ph√°t hi·ªán edges v√† h∆∞·ªõng c·ªßa ch√∫ng
        - B·∫•t bi·∫øn v·ªõi lighting
        
        **·ª®ng d·ª•ng:**
        - Object detection
        - Face recognition
        """)
    
    with col3:
        st.write("""
        **Edge Detection:**
        - T√¨m bi√™n c·ªßa objects
        - S·ª≠ d·ª•ng Canny algorithm
        
        **·ª®ng d·ª•ng:**
        - Shape detection
        - Image segmentation
        """)
    
    st.markdown("---")
    st.subheader("üéÆ Demo Interactive")
    
    # Upload image
    uploaded_file = st.file_uploader("Upload ·∫£nh c·ªßa b·∫°n:", type=['png', 'jpg', 'jpeg'])
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        method = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p:", ["histogram", "hog", "edges"])
    
    if uploaded_file is not None:
        # Load image
        image = Image.open(uploaded_file)
        
        with col2:
            st.write("**Original Image:**")
            st.image(image, width=300)
        
        if st.button("üöÄ Extract Image Features"):
            extractor = ImageFeatureExtractor(method=method)
            
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                features = extractor.extract_features(image)
                
                st.success("‚úÖ Extraction ho√†n t·∫•t!")
                
                # Show features
                st.subheader("üìä Extracted Features")
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.write(f"**Feature Vector Shape:** {features.shape}")
                    st.write(f"**Number of Features:** {len(features)}")
                    
                    st.write("**First 20 features:**")
                    st.write(features[:20])
                
                with col_b:
                    # Visualization
                    if method in ['histogram', 'edges']:
                        fig = extractor.visualize_features(image)
                        st.pyplot(fig)
                    elif method == 'hog':
                        st.write("**Feature Distribution:**")
                        import matplotlib.pyplot as plt
                        fig, ax = plt.subplots(figsize=(10, 3))
                        ax.plot(features[:100])
                        ax.set_title('First 100 HOG Features')
                        ax.set_xlabel('Feature Index')
                        ax.set_ylabel('Value')
                        st.pyplot(fig)
                
                # Download features
                st.download_button(
                    label="üì• Download Feature Vector",
                    data=features.tobytes(),
                    file_name=f"{method}_features.npy",
                    mime="application/octet-stream"
                )
                
                # Explanation
                with st.expander("üí° Gi·∫£i th√≠ch k·∫øt qu·∫£"):
                    if method == 'histogram':
                        st.write("""
                        **Color Histogram** cho th·∫•y ph√¢n b·ªë m√†u s·∫Øc trong ·∫£nh:
                        - Peaks cao = nhi·ªÅu pixels c√≥ m√†u ƒë√≥
                        - 3 histograms ri√™ng bi·ªát cho R, G, B channels
                        - Normalized v·ªÅ [0, 1] ƒë·ªÉ d·ªÖ so s√°nh
                        """)
                    elif method == 'hog':
                        st.write("""
                        **HOG Features** m√¥ t·∫£ shape v√† structure c·ªßa objects:
                        - T√≠nh gradient direction t·∫°i m·ªói pixel
                        - Chia ·∫£nh th√†nh cells v√† t√≠nh histogram
                        - Feature vector d√†i (th∆∞·ªùng >1000 dimensions)
                        """)
                    elif method == 'edges':
                        st.write("""
                        **Edge Features** highlight bi√™n c·ªßa objects:
                        - S·ª≠ d·ª•ng Canny edge detector
                        - Gi√° tr·ªã 1 = edge, 0 = kh√¥ng ph·∫£i edge
                        - Flattened th√†nh vector 1D
                        """)
    
    else:
        st.info("üëÜ Upload m·ªôt ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu!")
        
        # Show example
        st.write("**Ho·∫∑c th·ª≠ v·ªõi ·∫£nh m·∫´u:**")
        if st.button("S·ª≠ d·ª•ng ·∫£nh m·∫´u"):
            # Create sample image
            sample_image = np.random.randint(0, 255, (200, 200, 3), dtype=np.uint8)
            st.image(sample_image, caption="Sample Image", width=300)
            st.info("B·∫°n c√≥ th·ªÉ upload ·∫£nh ri√™ng c·ªßa b·∫°n ·ªü tr√™n!")

# ==================== SO S√ÅNH ====================
elif page == "üî¨ So s√°nh":
    st.header("So s√°nh 4 Ph∆∞∆°ng ph√°p Feature Extraction")
    
    st.subheader("üìä B·∫£ng so s√°nh t·ªïng quan")
    
    comparison_data = {
        'Ph∆∞∆°ng ph√°p': ['Dict Features', 'Feature Hashing', 'Text Features', 'Image Features'],
        'Input Type': ['Dictionary', 'Dictionary', 'Text', 'Image'],
        'Output Type': ['Dense Vector', 'Dense Vector', 'Sparse Vector', 'Dense Vector'],
        'S·ªë Features': ['T·ª± ƒë·ªông (= unique values)', 'C·ªë ƒë·ªãnh (user ƒë·ªãnh)', 'User ƒë·ªãnh (max_features)', 'Ph·ª• thu·ªôc method'],
        'T·ªëc ƒë·ªô': ['Nhanh', 'R·∫•t nhanh ‚ö°', 'Trung b√¨nh', 'Ch·∫≠m'],
        'B·ªô nh·ªõ': ['Trung b√¨nh', 'Th·∫•p üíæ', 'Cao', 'R·∫•t cao'],
        '·ª®ng d·ª•ng': ['Categorical data', 'Large vocabulary', 'NLP, Text Mining', 'Computer Vision']
    }
    
    df_comparison = pd.DataFrame(comparison_data)
    st.dataframe(df_comparison, use_container_width=True)
    
    st.markdown("---")
    
    # ∆Øu nh∆∞·ª£c ƒëi·ªÉm
    st.subheader("‚öñÔ∏è ∆Øu & Nh∆∞·ª£c ƒëi·ªÉm")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**1Ô∏è‚É£ Dict Features (DictVectorizer)**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - D·ªÖ hi·ªÉu, interpretable
        - Gi·ªØ nguy√™n feature names
        - Kh√¥ng b·ªã hash collision
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - T·ªën b·ªô nh·ªõ v·ªõi large vocabulary
        - C·∫ßn fit tr∆∞·ªõc khi transform
        - Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c unseen values
        """)
        
        st.write("**2Ô∏è‚É£ Feature Hashing**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - R·∫•t nhanh, scalable
        - Kh√¥ng c·∫ßn l∆∞u vocabulary
        - X·ª≠ l√Ω ƒë∆∞·ª£c unseen values
        - Ti·∫øt ki·ªám b·ªô nh·ªõ
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - Hash collision
        - M·∫•t interpretability
        - Kh√¥ng th·ªÉ reverse
        """)
    
    with col2:
        st.write("**3Ô∏è‚É£ Text Features (TF-IDF)**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - Ph·∫£n √°nh importance c·ªßa t·ª´
        - Gi·∫£m noise (stopwords)
        - Hi·ªáu qu·∫£ cho text classification
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - M·∫•t th·ª© t·ª± t·ª´ (bag of words)
        - Kh√¥ng hi·ªÉu ng·ªØ nghƒ©a
        - Sparse vector (t·ªën memory)
        """)
        
        st.write("**4Ô∏è‚É£ Image Features**")
        st.success("""
        ‚úÖ **∆Øu ƒëi·ªÉm:**
        - Capture visual information
        - Robust v·ªõi transformations
        - Nhi·ªÅu methods l·ª±a ch·ªçn
        """)
        st.error("""
        ‚ùå **Nh∆∞·ª£c ƒëi·ªÉm:**
        - T√≠nh to√°n ch·∫≠m
        - High dimensional
        - C·∫ßn preprocessing
        """)
    
    st.markdown("---")
    
    # Performance comparison
    st.subheader("‚ö° So s√°nh Performance")
    
    perf_data = {
        'Method': ['Dict', 'Hash', 'Text (TF-IDF)', 'Image (HOG)'],
        'Training Time': [0.01, 0.005, 0.5, 2.0],
        'Inference Time': [0.005, 0.002, 0.1, 1.5],
        'Memory Usage (MB)': [10, 5, 50, 100]
    }
    
    df_perf = pd.DataFrame(perf_data)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Training Time (seconds):**")
        st.bar_chart(df_perf.set_index('Method')['Training Time'])
    
    with col2:
        st.write("**Memory Usage (MB):**")
        st.bar_chart(df_perf.set_index('Method')['Memory Usage (MB)'])
    
    st.caption("*S·ªë li·ªáu mang t√≠nh ch·∫•t minh h·ªça v·ªõi dataset nh·ªè")
    
    st.markdown("---")
    
    # Khi n√†o d√πng c√°i g√¨
    st.subheader("üéØ Khi n√†o n√™n d√πng ph∆∞∆°ng ph√°p n√†o?")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.info("""
        **D√πng Dict Features khi:**
        - Dataset nh·ªè, s·ªë features kh√¥ng qu√° l·ªõn
        - C·∫ßn interpretability
        - C√≥ categorical data
        - V√≠ d·ª•: D·ªØ li·ªáu kh√°ch h√†ng, s·∫£n ph·∫©m
        """)
        
        st.info("""
        **D√πng Feature Hashing khi:**
        - Dataset r·∫•t l·ªõn (millions of features)
        - C·∫ßn speed & scalability
        - Kh√¥ng quan t√¢m interpretability
        - V√≠ d·ª•: Click-through rate prediction
        """)
    
    with col2:
        st.info("""
        **D√πng Text Features khi:**
        - L√†m vi·ªác v·ªõi vƒÉn b·∫£n
        - Text classification, sentiment analysis
        - C√≥ ƒë·ªß b·ªô nh·ªõ
        - V√≠ d·ª•: Spam detection, topic modeling
        """)
        
        st.info("""
        **D√πng Image Features khi:**
        - L√†m vi·ªác v·ªõi ·∫£nh
        - Computer vision tasks
        - C√≥ GPU (cho deep learning)
        - V√≠ d·ª•: Object detection, face recognition
        """)
    
    st.markdown("---")
    
    # Summary
    st.subheader("üìù T√≥m t·∫Øt")
    st.write("""
    **Kh√¥ng c√≥ ph∆∞∆°ng ph√°p n√†o l√† t·ªët nh·∫•t cho m·ªçi tr∆∞·ªùng h·ª£p!**
    
    L·ª±a ch·ªçn ph∆∞∆°ng ph√°p ph·ª• thu·ªôc v√†o:
    1. **Lo·∫°i d·ªØ li·ªáu:** Categorical, Text, Image?
    2. **K√≠ch th∆∞·ªõc dataset:** L·ªõn hay nh·ªè?
    3. **Y√™u c·∫ßu:** Speed, Accuracy, Interpretability?
    4. **Resources:** Memory, CPU, GPU?
    
    üí° **Best practice:** Th·ª≠ nhi·ªÅu methods v√† so s√°nh k·∫øt qu·∫£!
    """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>üéì Demo by Group 8 - M√¥n Khai Ph√° D·ªØ Li·ªáu</p>
    <p>Made with ‚ù§Ô∏è using Streamlit</p>
</div>
""", unsafe_allow_html=True)
