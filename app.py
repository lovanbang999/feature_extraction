"""
WEB APP FEATURE EXTRACTION DEMO
Streamlit application tÃ­ch há»£p 4 phÆ°Æ¡ng phÃ¡p feature extraction
"""

import streamlit as st
import pandas as pd
import numpy as np
from PIL import Image
import sys
import os

# Custom CSS
st.markdown("""
<style>
    .main > div {
        padding-top: 2rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #FF4B4B;
        color: white;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #FF6B6B;
    }
    h1 {
        color: #FF4B4B;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #FF4B4B;
    }
</style>
""", unsafe_allow_html=True)

# Add utils to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'utils'))

from dict_features import DictFeatureExtractor
from hash_features import HashFeatureExtractor
from text_features import TextFeatureExtractor
from image_features import ImageFeatureExtractor

# Page config
st.set_page_config(
    page_title="Feature Extraction Demo",
    page_icon="ğŸš€",
    layout="wide"
)

with st.expander("ğŸ‘¥ ThÃ´ng tin nhÃ³m", expanded=False):
    st.write("**NhÃ³m 8 - MÃ´n Khai PhÃ¡ Dá»¯ Liá»‡u**")
    
    team_members = [
        {'STT': 1, 'Há» tÃªn': 'LÃ² VÄƒn Báº±ng', 'MSSV': '2251061721', 'Pháº§n Ä‘áº£m nháº­n': 'Image Features'},
        {'STT': 2, 'Há» tÃªn': 'Nguyá»…n Trung KiÃªn', 'MSSV': '2251061811', 'Pháº§n Ä‘áº£m nháº­n': 'Dict Features'},
        {'STT': 3, 'Há» tÃªn': 'Thiá»u BÃ¡ Viá»‡t', 'MSSV': '2251061924', 'Pháº§n Ä‘áº£m nháº­n': 'Text Features'},
        {'STT': 4, 'Há» tÃªn': 'LÆ°á»ng VÄƒn CÆ°Æ¡ng', 'MSSV': '20210004', 'Pháº§n Ä‘áº£m nháº­n': 'Feature Hashing'}
    ]
    
    df_team = pd.DataFrame(team_members)
    st.dataframe(df_team, hide_index=True, use_container_width=True)

# Title
st.title("ğŸš€ Feature Extraction Demo")
st.markdown("**NhÃ³m 8 - MÃ´n Khai PhÃ¡ Dá»¯ Liá»‡u**")
st.markdown("---")

# Sidebar
st.sidebar.title("ğŸ“‹ Navigation")
page = st.sidebar.radio(
    "Chá»n phÆ°Æ¡ng phÃ¡p:",
    ["ğŸ  Tá»•ng quan", "ğŸ“Š Dict Features", "# Feature Hashing", 
     "ğŸ“ Text Features", "ğŸ–¼ï¸ Image Features"]
)

# ==================== TRANG Tá»”NG QUAN ====================
if page == "ğŸ  Tá»•ng quan":
    st.header("Giá»›i thiá»‡u Feature Extraction")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“– Feature Extraction lÃ  gÃ¬?")
        st.write("""
        Feature Extraction (TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng) lÃ  quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ´ 
        (raw data) thÃ nh dáº¡ng sá»‘ (numerical) mÃ  cÃ¡c thuáº­t toÃ¡n machine learning cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c.
        
        **Táº¡i sao cáº§n Feature Extraction?**
        - Machine learning chá»‰ lÃ m viá»‡c vá»›i sá»‘
        - Giáº£m chiá»u dá»¯ liá»‡u (dimensionality reduction)
        - Giá»¯ láº¡i thÃ´ng tin quan trá»ng
        - Cáº£i thiá»‡n hiá»‡u suáº¥t model
        """)
    
    with col2:
        st.subheader("ğŸ¯ 4 PhÆ°Æ¡ng phÃ¡p trong Demo")
        st.write("""
        1. **Dict Features**: Chuyá»ƒn dictionary â†’ vector
        2. **Feature Hashing**: Hash trick cho high-dimensional data
        3. **Text Features**: Chuyá»ƒn text â†’ vector (TF-IDF, Count)
        4. **Image Features**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« áº£nh
        """)
    
    st.markdown("---")
    st.subheader("ğŸ“Š Quy trÃ¬nh chung")
    st.image("https://miro.medium.com/max/1400/1*VQvV5kVXZvHlmBDxWcZvNg.png", 
             caption="Feature Extraction Pipeline", use_column_width=True)
    
    st.info("ğŸ‘ˆ Chá»n phÆ°Æ¡ng phÃ¡p á»Ÿ sidebar Ä‘á»ƒ báº¯t Ä‘áº§u demo!")

# ==================== DICT FEATURES ====================
elif page == "ğŸ“Š Dict Features":
    st.header("7.2.1. Loading Features from Dicts")
    
    st.subheader("ğŸ“š LÃ½ thuyáº¿t")
    st.write("""
    **DictVectorizer** chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u dáº¡ng dictionary thÃ nh feature vectors.
    
    **VÃ­ dá»¥:**
```python
    Input:  [{'city': 'Hanoi', 'age': 25}, {'city': 'HCM', 'age': 30}]
    Output: [[25, 0, 1], [30, 1, 0]]  # [age, city=HCM, city=Hanoi]
```
    
    **á»¨ng dá»¥ng:** Dá»¯ liá»‡u categorical nhÆ° thÃ´ng tin khÃ¡ch hÃ ng, sáº£n pháº©m, v.v.
    """)
    
    st.markdown("---")
    st.subheader("ğŸ® Demo Interactive")
    
    # Chá»n mode
    demo_mode = st.radio("Chá»n mode:", ["VÃ­ dá»¥ Ä‘Æ¡n giáº£n", "Titanic Dataset"])
    
    if demo_mode == "VÃ­ dá»¥ Ä‘Æ¡n giáº£n":
        st.write("**Nháº­p dá»¯ liá»‡u cá»§a báº¡n:**")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            name1 = st.text_input("TÃªn 1:", "An")
            city1 = st.selectbox("ThÃ nh phá»‘ 1:", ["Hanoi", "HCM", "Danang"], key="city1")
            age1 = st.number_input("Tuá»•i 1:", 18, 100, 25)
        
        with col2:
            name2 = st.text_input("TÃªn 2:", "Binh")
            city2 = st.selectbox("ThÃ nh phá»‘ 2:", ["Hanoi", "HCM", "Danang"], key="city2")
            age2 = st.number_input("Tuá»•i 2:", 18, 100, 30)
        
        with col3:
            name3 = st.text_input("TÃªn 3:", "Chi")
            city3 = st.selectbox("ThÃ nh phá»‘ 3:", ["Hanoi", "HCM", "Danang"], key="city3")
            age3 = st.number_input("Tuá»•i 3:", 18, 100, 22)
        
        if st.button("ğŸš€ Extract Features"):
            data = [
                {'name': name1, 'city': city1, 'age': age1},
                {'name': name2, 'city': city2, 'age': age2},
                {'name': name3, 'city': city3, 'age': age3}
            ]
            
            extractor = DictFeatureExtractor()
            features, feature_names = extractor.extract_features(data)
            
            st.success("âœ… Extraction hoÃ n táº¥t!")
            
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.write("**Input Data:**")
                st.json(data)
            
            with col_b:
                st.write("**Feature Names:**")
                st.write(list(feature_names))
            
            st.write("**Feature Matrix:**")
            df_features = pd.DataFrame(features, columns=feature_names)
            st.dataframe(df_features, use_container_width=True)
            
            st.write(f"ğŸ“ **Shape:** {features.shape} (3 samples Ã— {features.shape[1]} features)")
    
    else:  # Titanic Dataset
        st.write("**Demo vá»›i Titanic Dataset:**")
        
        try:
            df = pd.read_csv('datasets/titanic.csv')
            st.write("Dataset preview:")
            st.dataframe(df.head(), use_container_width=True)
            
            n_samples = st.slider("Sá»‘ lÆ°á»£ng samples:", 5, 50, 10)
            
            if st.button("ğŸš€ Extract Features from Titanic"):
                extractor = DictFeatureExtractor()
                dict_data, features, feature_names = extractor.demo_with_titanic(df, n_samples)
                
                st.success(f"âœ… ÄÃ£ extract {n_samples} samples!")
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.write("**Original Data (dict format):**")
                    st.json(dict_data[:3])  # Show first 3
                
                with col_b:
                    st.write("**Feature Names:**")
                    st.write(list(feature_names))
                
                st.write("**Feature Matrix:**")
                df_features = pd.DataFrame(features, columns=feature_names)
                st.dataframe(df_features, use_container_width=True)
                
                st.write(f"ğŸ“ **Shape:** {features.shape}")
        
        except FileNotFoundError:
            st.error("âŒ KhÃ´ng tÃ¬m tháº¥y datasets/titanic.csv. Vui lÃ²ng cháº¡y download_data.py trÆ°á»›c!")

# ==================== FEATURE HASHING ====================
elif page == "# Feature Hashing":
    st.header("7.2.2. Feature Hashing")
    
    st.subheader("ğŸ“š LÃ½ thuyáº¿t")
    st.write("""
    **Feature Hashing** (Hashing Trick) sá»­ dá»¥ng hash function Ä‘á»ƒ chuyá»ƒn features thÃ nh vector 
    cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh.
    
    **Æ¯u Ä‘iá»ƒm:**
    - âš¡ Ráº¥t nhanh (khÃ´ng cáº§n lÆ°u vocabulary)
    - ğŸ’¾ Tiáº¿t kiá»‡m bá»™ nhá»›
    - ğŸ”„ Xá»­ lÃ½ Ä‘Æ°á»£c unseen features
    
    **NhÆ°á»£c Ä‘iá»ƒm:**
    - âš ï¸ Hash collision (nhiá»u features â†’ cÃ¹ng 1 hash value)
    - â“ KhÃ´ng biáº¿t feature gá»‘c lÃ  gÃ¬ (one-way)
    """)
    
    st.markdown("---")
    st.subheader("ğŸ® Demo Interactive")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        country1 = st.text_input("Country 1:", "Vietnam")
        city1 = st.text_input("City 1:", "Hanoi")
        age1 = st.number_input("Age 1:", 18, 100, 25, key="hash_age1")
    
    with col2:
        country2 = st.text_input("Country 2:", "Thailand")
        city2 = st.text_input("City 2:", "Bangkok")
        age2 = st.number_input("Age 2:", 18, 100, 28, key="hash_age2")
    
    with col3:
        country3 = st.text_input("Country 3:", "Vietnam")
        city3 = st.text_input("City 3:", "HCM")
        age3 = st.number_input("Age 3:", 18, 100, 30, key="hash_age3")
    
    n_features = st.slider("Sá»‘ features sau khi hash:", 5, 20, 10)
    
    if st.button("ğŸš€ Extract & Compare"):
        data = [
            {'country': country1, 'city': city1, 'age': age1},
            {'country': country2, 'city': city2, 'age': age2},
            {'country': country3, 'city': city3, 'age': age3}
        ]
        
        extractor = HashFeatureExtractor(n_features=n_features)
        comparison, dict_features, hash_features = extractor.compare_with_dict_vectorizer(data)
        
        st.success("âœ… Extraction hoÃ n táº¥t!")
        
        # Show original data
        st.write("**Input Data:**")
        st.json(data)
        
        # Comparison table
        st.subheader("ğŸ“Š So sÃ¡nh DictVectorizer vs FeatureHasher")
        
        comp_df = pd.DataFrame({
            'Method': ['DictVectorizer', 'FeatureHasher'],
            'Number of Features': [comparison['DictVectorizer']['n_features'], 
                                   comparison['FeatureHasher']['n_features']],
            'Memory (bytes)': [comparison['DictVectorizer']['memory_size'],
                              comparison['FeatureHasher']['memory_size']]
        })
        st.dataframe(comp_df, use_container_width=True)
        
        # Show features
        col_a, col_b = st.columns(2)
        
        with col_a:
            st.write("**DictVectorizer Output:**")
            st.write(dict_features)
            st.caption(f"Shape: {dict_features.shape}")
        
        with col_b:
            st.write("**FeatureHasher Output:**")
            st.write(hash_features)
            st.caption(f"Shape: {hash_features.shape}")
        
        st.info("""
        **ğŸ’¡ Nháº­n xÃ©t:**
        - FeatureHasher cÃ³ sá»‘ features cá»‘ Ä‘á»‹nh (báº¡n chá»n)
        - DictVectorizer cÃ³ sá»‘ features = sá»‘ unique values
        - FeatureHasher tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n khi cÃ³ nhiá»u categorical values
        """)

# ==================== TEXT FEATURES ====================
elif page == "ğŸ“ Text Features":
    st.header("7.2.3. Text Feature Extraction")
    
    st.subheader("ğŸ“š LÃ½ thuyáº¿t")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **Count Vectorizer:**
        - Äáº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»—i tá»«
        - VÃ­ dá»¥: "I love ML" â†’ [1, 1, 1]
        
        **Æ¯u Ä‘iá»ƒm:**
        - ÄÆ¡n giáº£n, dá»… hiá»ƒu
        - PhÃ¹ há»£p vá»›i short texts
        """)
    
    with col2:
        st.write("""
        **TF-IDF Vectorizer:**
        - TF (Term Frequency): Táº§n suáº¥t tá»« trong document
        - IDF (Inverse Document Frequency): Äá»™ quan trá»ng cá»§a tá»«
        - TF-IDF = TF Ã— IDF
        
        **Æ¯u Ä‘iá»ƒm:**
        - Pháº£n Ã¡nh táº§m quan trá»ng cá»§a tá»«
        - Giáº£m trá»ng sá»‘ cá»§a tá»« phá»• biáº¿n (the, is, a,...)
        """)
    
    st.markdown("---")
    st.subheader("ğŸ® Demo Interactive")
    
    # Mode selection
    demo_mode = st.radio("Chá»n mode:", ["Nháº­p text tá»± do", "Máº«u cÃ³ sáºµn"], horizontal=True)
    
    if demo_mode == "Nháº­p text tá»± do":
        st.write("**Nháº­p cÃ¡c Ä‘oáº¡n text (má»—i dÃ²ng = 1 document):**")
        
        text_input = st.text_area(
            "Your texts:",
            value="I love machine learning\nPython is great for data science\nDeep learning requires GPUs",
            height=150
        )
        
        texts = [t.strip() for t in text_input.split('\n') if t.strip()]
    
    else:
        texts = [
            "Machine learning is a subset of artificial intelligence",
            "Data science combines statistics and programming",
            "Python is the most popular language for machine learning",
            "Deep learning uses neural networks with multiple layers",
            "Natural language processing deals with text and speech"
        ]
        st.write("**Sample texts:**")
        for i, text in enumerate(texts, 1):
            st.write(f"{i}. {text}")
    
    # Parameters
    col1, col2, col3 = st.columns(3)
    with col1:
        method = st.selectbox("Method:", ["tfidf", "count", "hashing"])
    with col2:
        max_features = st.slider("Max features:", 10, 50, 20)
    with col3:
        show_top = st.slider("Show top words:", 5, 20, 10)
    
    if st.button("ğŸš€ Extract Text Features"):
        extractor = TextFeatureExtractor(method=method, max_features=max_features)
        features, feature_names, top_words = extractor.analyze_text(texts)
        
        st.success("âœ… Extraction hoÃ n táº¥t!")
        
        # Feature matrix
        st.subheader("ğŸ“Š Feature Matrix")
        df_features = pd.DataFrame(features, columns=feature_names)
        df_features.index = [f"Doc {i+1}" for i in range(len(texts))]
        st.dataframe(df_features, use_container_width=True)
        
        st.write(f"ğŸ“ **Shape:** {features.shape} ({len(texts)} documents Ã— {features.shape[1]} words)")
        
        # Top words
        col_a, col_b = st.columns([1, 2])
        
        with col_a:
            st.subheader(f"ğŸ” Top {show_top} Words")
            top_df = pd.DataFrame(top_words[:show_top], columns=['Word', 'Score'])
            st.dataframe(top_df, use_container_width=True)
        
        with col_b:
            st.subheader("ğŸ“ˆ Visualization")
            fig = extractor.visualize_features(texts, feature_names, features)
            st.plotly_chart(fig, use_container_width=True)
        
        # Explanation
        with st.expander("ğŸ’¡ Giáº£i thÃ­ch káº¿t quáº£"):
            if method == 'tfidf':
                st.write("""
                **TF-IDF scores cao** = tá»« quan trá»ng trong document vÃ  Ã­t xuáº¥t hiá»‡n trong cÃ¡c documents khÃ¡c
                
                VÃ­ dá»¥:
                - "learning", "machine" cÃ³ score cao vÃ¬ xuáº¥t hiá»‡n nhiá»u láº§n
                - "the", "is", "a" bá»‹ loáº¡i bá» (stopwords) hoáº·c cÃ³ score tháº¥p
                """)
            else:
                st.write("""
                **Count values** = sá»‘ láº§n tá»« xuáº¥t hiá»‡n trong má»—i document
                
                - Count = 3: tá»« xuáº¥t hiá»‡n 3 láº§n
                - Count = 0: tá»« khÃ´ng xuáº¥t hiá»‡n
                """)

# ==================== IMAGE FEATURES ====================
elif page == "ğŸ–¼ï¸ Image Features":
    st.header("7.2.4. Image Feature Extraction (Scikit-learn)")
    
    st.info("âš ï¸ **Theo tÃ i liá»‡u scikit-learn**, Image Feature Extraction bao gá»“m: **Patch Extraction** vÃ  **Image-to-Graph Conversion**")
    
    st.subheader("ğŸ“š LÃ½ thuyáº¿t")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("""
        **1ï¸âƒ£ Patch Extraction:**
        - Chia áº£nh thÃ nh patches nhá»
        - Há»c local features tá»« raw pixels
        - API: `extract_patches_2d()`, `PatchExtractor`
        
        **á»¨ng dá»¥ng:**
        - Dictionary Learning
        - Image Denoising
        - Texture Analysis
        """)
    
    with col2:
        st.write("""
        **2ï¸âƒ£ Image-to-Graph:**
        - Chuyá»ƒn áº£nh thÃ nh graph structure
        - Má»—i pixel = 1 node trong graph
        - API: `img_to_graph()`
        
        **á»¨ng dá»¥ng:**
        - Spectral Clustering
        - Image Segmentation
        - Region Analysis
        """)
    
    st.markdown("---")
    st.subheader("ğŸ® Demo Interactive")
    
    # Import sklearn version
    from utils.image_features import ImageFeatureExtractor
    
    # Upload or select image
    uploaded_file = st.file_uploader("Upload áº£nh cá»§a báº¡n:", type=['png', 'jpg', 'jpeg'])
    
    st.write("**Hoáº·c chá»n áº£nh máº«u:**")
    sample_choice = st.selectbox(
        "Chá»n áº£nh máº«u:",
        ["KhÃ´ng chá»n", "Red Image", "Green Image", "Pattern Image", "Gradient Image"]
    )
    
    # Xá»­ lÃ½ áº£nh
    image = None
    if sample_choice != "KhÃ´ng chá»n":
        sample_map = {
            "Red Image": "datasets/sample_images/red_image.png",
            "Green Image": "datasets/sample_images/green_image.png",
            "Pattern Image": "datasets/sample_images/pattern_image.png",
            "Gradient Image": "datasets/sample_images/gradient_image.png"
        }
        
        sample_path = sample_map[sample_choice]
        if os.path.exists(sample_path):
            image = Image.open(sample_path)
        else:
            st.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y áº£nh máº«u: {sample_path}")
    
    elif uploaded_file is not None:
        image = Image.open(uploaded_file)
    
    # Method selection & parameters
    col1, col2 = st.columns([1, 2])
    
    with col1:
        method = st.radio("Chá»n phÆ°Æ¡ng phÃ¡p:", ["patches", "graph"])
        
        if method == "patches":
            st.write("**Tham sá»‘ Patch Extraction:**")
            patch_size = st.slider("Patch size:", 16, 64, 32, 8)
            max_patches = st.slider("Max patches:", 20, 200, 100, 10)
        else:
            st.write("**Tham sá»‘ Graph Clustering:**")
            n_clusters = st.slider("Number of clusters:", 2, 8, 3)
    
    if image is not None:
        with col2:
            st.write("**Original Image:**")
            st.image(image, width=300)
            image_array = np.array(image)
            st.caption(f"Size: {image_array.shape[1]}Ã—{image_array.shape[0]} pixels")
        
        if st.button("ğŸš€ Extract Features (scikit-learn)"):
            if method == "patches":
                # ========== PATCH EXTRACTION ==========
                extractor = ImageFeatureExtractor(method='patches')
                
                with st.spinner("Extracting patches..."):
                    features, extras = extractor.extract_features(
                        image,
                        patch_size=(patch_size, patch_size),
                        max_patches=max_patches
                    )
                    
                    st.success("âœ… Patch Extraction hoÃ n táº¥t!")
                    
                    # Show info
                    st.subheader("ğŸ“Š Káº¿t Quáº£ Patch Extraction")
                    
                    # Metrics
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Number of Patches", extras['n_patches'])
                    with col_b:
                        st.metric("Patch Size", f"{patch_size}Ã—{patch_size}")
                    with col_c:
                        st.metric("Features per Patch", patch_size * patch_size * 3)
                    
                    # Details
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write("**ğŸ“‹ ThÃ´ng tin chi tiáº¿t:**")
                        st.write(f"- Patches shape: `{extras['patches'].shape}`")
                        st.write(f"- Flattened features: `{features.shape}`")
                        st.write(f"- Total features: `{features.shape[0] * features.shape[1]:,}`")
                        
                        st.write("\n**ğŸ”¢ First patch (first 20 features):**")
                        st.write(features[0][:20])
                        
                        # Explain patch calculation
                        with st.expander("ğŸ’¡ CÃ¡ch tÃ­nh sá»‘ patches"):
                            st.write(f"""
                            **PhÆ°Æ¡ng phÃ¡p: Random Sampling**
                            
                            - áº¢nh gá»‘c: {image_array.shape[1]}Ã—{image_array.shape[0]}
                            - Patch size: {patch_size}Ã—{patch_size}
                            - `max_patches={max_patches}` â†’ Random sampling {max_patches} vá»‹ trÃ­
                            
                            **Náº¿u cáº¯t Ä‘á»u (non-overlapping):**
                            - Sá»‘ patches ngang: {image_array.shape[1] // patch_size}
                            - Sá»‘ patches dá»c: {image_array.shape[0] // patch_size}
                            - Tá»•ng: {(image_array.shape[1] // patch_size) * (image_array.shape[0] // patch_size)} patches
                            
                            **Vá»›i random sampling:**
                            - Chá»n ngáº«u nhiÃªn {max_patches} vá»‹ trÃ­
                            - CÃ³ thá»ƒ overlap (chá»“ng láº¥n)
                            - Linh hoáº¡t hÆ¡n cho Dictionary Learning
                            """)
                    
                    with col_b:
                        st.write("**ğŸ–¼ï¸ Visualization (16 patches Ä‘áº§u tiÃªn):**")
                        fig = extractor.visualize_patches(extras['patches'], n_display=16)
                        st.pyplot(fig)
                    
                    # Explanation
                    with st.expander("ğŸ“– Giáº£i thÃ­ch Patch Extraction"):
                        st.write("""
                        **Patch Extraction lÃ  gÃ¬?**
                        
                        Chia áº£nh lá»›n thÃ nh nhiá»u patches (máº£nh) nhá» Ä‘á»ƒ há»c cÃ¡c local patterns.
                        
                        **Quy trÃ¬nh:**
                        1. Chá»n patch size (vÃ­ dá»¥: 32Ã—32)
                        2. Random sampling hoáº·c sliding window
                        3. Extract tá»«ng patch thÃ nh vector
                        4. Flatten: 32Ã—32Ã—3 = 3,072 features/patch
                        
                        **á»¨ng dá»¥ng thá»±c táº¿:**
                        - **Dictionary Learning**: Há»c "alphabet" cá»§a hÃ¬nh áº£nh
                        - **Image Denoising**: Khá»­ nhiá»…u báº±ng cÃ¡ch so sÃ¡nh patches
                        - **Texture Recognition**: PhÃ¢n loáº¡i textures (gá»—, váº£i, Ä‘Ã¡...)
                        - **Feature Extraction**: DÃ¹ng lÃ m input cho ML models
                        
                        **Tham kháº£o:**
                        - [sklearn.feature_extraction.image.extract_patches_2d](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html)
                        """)
            
            else:
                # ========== IMAGE-TO-GRAPH ==========
                extractor = ImageFeatureExtractor(method='graph')
                
                with st.spinner("Converting to graph & clustering..."):
                    labels, extras = extractor.extract_features(
                        image,
                        n_clusters=n_clusters
                    )
                    
                    st.success("âœ… Image-to-Graph Conversion hoÃ n táº¥t!")
                    
                    # Show info
                    st.subheader("ğŸ“Š Káº¿t Quáº£ Graph-based Segmentation")
                    
                    # Metrics
                    col_a, col_b, col_c = st.columns(3)
                    with col_a:
                        st.metric("Number of Nodes", extras['graph'].shape[0])
                    with col_b:
                        st.metric("Number of Clusters", n_clusters)
                    with col_c:
                        st.metric("Graph Edges", extras['graph'].nnz // 2)
                    
                    # Details
                    col_a, col_b = st.columns(2)
                    
                    with col_a:
                        st.write("**ğŸ“‹ ThÃ´ng tin Graph:**")
                        st.write(f"- Graph shape: `{extras['graph'].shape}`")
                        st.write(f"- Graph type: Sparse adjacency matrix")
                        st.write(f"- Labels shape: `{labels.shape}`")
                        st.write(f"- Segmented shape: `{extras['segmented'].shape}`")
                        
                        st.write("\n**ğŸ“Š Cluster Distribution:**")
                        unique, counts = np.unique(labels, return_counts=True)
                        cluster_data = []
                        for cluster, count in zip(unique, counts):
                            percentage = (count / len(labels)) * 100
                            cluster_data.append({
                                'Cluster': cluster,
                                'Pixels': count,
                                'Percentage': f"{percentage:.1f}%"
                            })
                        st.dataframe(pd.DataFrame(cluster_data), hide_index=True, use_container_width=True)
                        
                        with st.expander("ğŸ’¡ Táº¡i sao resize vá» 50Ã—50?"):
                            st.write("""
                            Graph-based clustering **ráº¥t cháº­m** vá»›i áº£nh lá»›n vÃ¬:
                            - áº¢nh 256Ã—256 = 65,536 nodes â†’ Ma tráº­n 65,536 Ã— 65,536!
                            - Spectral clustering complexity: O(nÂ³)
                            
                            Resize vá» 50Ã—50:
                            - 2,500 nodes â†’ Nhanh hÆ¡n nhiá»u
                            - Váº«n giá»¯ Ä‘Æ°á»£c structure chÃ­nh cá»§a áº£nh
                            - PhÃ¹ há»£p cho demo & education
                            """)
                    
                    with col_b:
                        st.write("**ğŸ–¼ï¸ Segmentation Result:**")
                        fig = extractor.visualize_segmentation(
                            image_array,
                            extras['segmented'],
                            extras['small_image']
                        )
                        st.pyplot(fig)
                    
                    # Explanation
                    with st.expander("ğŸ“– Giáº£i thÃ­ch Image-to-Graph"):
                        st.write("""
                        **Image-to-Graph lÃ  gÃ¬?**
                        
                        Chuyá»ƒn Ä‘á»•i hÃ¬nh áº£nh thÃ nh cáº¥u trÃºc Ä‘á»“ thá»‹ Ä‘á»ƒ phÃ¢n tÃ­ch relationships giá»¯a cÃ¡c pixels.
                        
                        **Cáº¥u trÃºc Graph:**
                        - **Nodes**: Má»—i pixel = 1 node
                        - **Edges**: Káº¿t ná»‘i vá»›i 4 hoáº·c 8 neighbors
                        - **Weights**: Dá»±a trÃªn Ä‘á»™ khÃ¡c biá»‡t mÃ u sáº¯c
                        
                        **Spectral Clustering:**
                        1. Build graph tá»« áº£nh
                        2. TÃ­nh eigenvectors cá»§a Laplacian matrix
                        3. Clustering trong eigenspace
                        4. GÃ¡n labels vá» pixels
                        
                        **á»¨ng dá»¥ng thá»±c táº¿:**
                        - **Medical Imaging**: PhÃ¢n vÃ¹ng cÆ¡ quan, tumor
                        - **Image Segmentation**: TÃ¡ch object khá»i background
                        - **Region Analysis**: PhÃ¢n tÃ­ch tá»«ng vÃ¹ng riÃªng biá»‡t
                        - **Interactive Selection**: Click Ä‘á»ƒ select region
                        
                        **Æ¯u Ä‘iá»ƒm:**
                        - Unsupervised (khÃ´ng cáº§n labels)
                        - Tá»± Ä‘á»™ng tÃ¬m boundaries
                        - Consider cáº£ color vÃ  spatial proximity
                        
                        **Tham kháº£o:**
                        - [sklearn.feature_extraction.image.img_to_graph](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html)
                        - [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)
                        """)
    
    else:
        st.info("ğŸ‘† Upload má»™t áº£nh hoáº·c chá»n áº£nh máº«u Ä‘á»ƒ báº¯t Ä‘áº§u!")
        
        # Documentation links
        st.markdown("---")
        st.write("**ğŸ“š TÃ i liá»‡u tham kháº£o chÃ­nh thá»©c:**")
        st.markdown("""
        - [Scikit-learn Feature Extraction Documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction)
        - [`sklearn.feature_extraction.image.extract_patches_2d`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.extract_patches_2d.html)
        - [`sklearn.feature_extraction.image.PatchExtractor`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.PatchExtractor.html)
        - [`sklearn.feature_extraction.image.img_to_graph`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.image.img_to_graph.html)
        - [Spectral Clustering](https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering)
        """)
        
        st.write("**ğŸ’¡ LÆ°u Ã½:**")
        st.info("""
        ÄÃ¢y lÃ  Image Feature Extraction theo **tÃ i liá»‡u scikit-learn chÃ­nh thá»©c**, 
        khÃ¡c vá»›i Computer Vision truyá»n thá»‘ng (Color Histogram, HOG, SIFT, CNN features).
        """)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center'>
    <p>ğŸ“ Demo by Group 8 - MÃ´n Khai PhÃ¡ Dá»¯ Liá»‡u</p>
    <p>Made with â¤ï¸ using Streamlit</p>
</div>
""", unsafe_allow_html=True)
